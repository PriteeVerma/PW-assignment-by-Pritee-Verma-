{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **KNN & PCA | Assignment**"
      ],
      "metadata": {
        "id": "WZnujRigPrPM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 1: What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?**\n",
        "\n",
        "#**Answer:**\n",
        "\n",
        "K-Nearest Neighbors (KNN) is a supervised machine learning algorithm used for classification and regression tasks. It is a non-parametric and lazy learning algorithm, which means it does not build an explicit model during training. Instead, it stores all the training data and makes predictions when a new data point is given.\n",
        "\n",
        "**How KNN Works:**\n",
        "-\n",
        "\n",
        "\n",
        "1. Select the value of K (number of nearest neighbors).\n",
        "\n",
        "2. Calculate the distance between the test data point and all training data points using a distance metric such as Euclidean distance.\n",
        "\n",
        "3. Identify the K closest data points to the test point.\n",
        "\n",
        "4. Use these neighbors to make the final prediction.\n",
        "\n",
        "**KNN in Classification:**\n",
        "-\n",
        "\n",
        "In classification problems, KNN assigns the class that is most frequent among the K nearest neighbors. This process is known as majority voting.\n",
        "For example, if 4 out of 5 nearest neighbors belong to Class A, the new data point is classified as Class A.\n",
        "\n",
        "**KNN in Regression:**\n",
        "-\n",
        "\n",
        "In regression problems, KNN predicts a continuous value by calculating the average (or weighted average) of the target values of the K nearest neighbors.\n",
        "\n",
        "**Advantages of KNN:**\n",
        "-\n",
        "\n",
        "- Simple and easy to understand\n",
        "\n",
        "- No training phase required\n",
        "\n",
        "- Works well with small datasets\n",
        "\n",
        "**Limitations of KNN:**\n",
        "-\n",
        "\n",
        "- Computationally expensive for large datasets\n",
        "\n",
        "- Sensitive to feature scaling\n",
        "\n",
        "- Performance degrades in high-dimensional data\n",
        "\n",
        "In summary, KNN is a powerful and intuitive algorithm that makes predictions based on similarity between data points and is widely used for both classification and regression tasks."
      ],
      "metadata": {
        "id": "4LwqPSvOPvk2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 2: What is the Curse of Dimensionality and how does it affect KNN performance?**\n",
        "\n",
        "#**Answer:**\n",
        "\n",
        "The Curse of Dimensionality refers to the problems that occur when the number of features (dimensions) in a dataset becomes very large. As dimensions increase, the data points become more sparse, and it becomes difficult for machine learning algorithms to find meaningful patterns.\n",
        "\n",
        "**Effect on KNN Performance:**\n",
        "-\n",
        "\n",
        "KNN relies on distance calculations to find nearest neighbors. In high-dimensional spaces:\n",
        "\n",
        "- The distance between data points becomes almost the same.\n",
        "\n",
        "- The concept of “nearest” neighbor loses its meaning.\n",
        "\n",
        "- KNN may select incorrect neighbors.\n",
        "\n",
        "- Model accuracy decreases.\n",
        "\n",
        "- Computation time increases significantly.\n",
        "\n",
        "**Why This Happens:**\n",
        "-\n",
        "\n",
        "- With more dimensions, the volume of the data space increases exponentially.\n",
        "\n",
        "- Available data becomes insufficient to cover the space properly.\n",
        "\n",
        "- Noise and irrelevant features dominate distance calculations.\n",
        "\n",
        "**Impact on KNN:**\n",
        "-\n",
        "\n",
        "- Poor classification or regression results\n",
        "\n",
        "- Increased overfitting\n",
        "\n",
        "- Slower prediction time\n",
        "\n",
        "**How to Handle the Curse of Dimensionality:**\n",
        "-\n",
        "\n",
        "- Apply feature scaling\n",
        "\n",
        "- Use dimensionality reduction techniques like PCA\n",
        "\n",
        "- Remove irrelevant or redundant features\n",
        "\n",
        "In conclusion, the Curse of Dimensionality negatively affects KNN by making distance-based learning unreliable, especially in high-dimensional datasets."
      ],
      "metadata": {
        "id": "h0VmIEMoRPZ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 3: What is Principal Component Analysis (PCA)? How is it different from feature selection?**\n",
        "\n",
        "#**Answer:**\n",
        "\n",
        "Principal Component Analysis (PCA) is an unsupervised machine learning technique used for dimensionality reduction. It transforms the original features into a new set of features called principal components. These components are uncorrelated and arranged in such a way that the first few components capture the maximum variance present in the data.\n",
        "\n",
        "**PCA helps in:**\n",
        "-\n",
        "\n",
        "- Reducing the number of features\n",
        "\n",
        "- Removing noise and redundancy\n",
        "\n",
        "- Improving model performance and computational efficiency\n",
        "\n",
        "**How PCA Works (Brief):**\n",
        "-\n",
        "\n",
        "- Data is standardized\n",
        "\n",
        "- Covariance matrix is computed\n",
        "\n",
        "- Eigenvalues and eigenvectors are calculated\n",
        "\n",
        "- Principal components are selected based on maximum variance\n",
        "\n",
        "**Difference Between PCA and Feature Selection:**\n",
        "-\n",
        "\n",
        "| PCA                                  | Feature Selection                        |\n",
        "| ------------------------------------ | ---------------------------------------- |\n",
        "| Creates new transformed features     | Selects a subset of original features    |\n",
        "| Unsupervised method                  | Can be supervised or unsupervised        |\n",
        "| Features lose original meaning       | Original meaning of features is retained |\n",
        "| Reduces correlation between features | Does not remove correlation              |\n",
        "\n",
        "**Conclusion:**\n",
        "-\n",
        "\n",
        "PCA reduces dimensionality by transforming data, while feature selection reduces dimensionality by choosing important features. Both methods aim to improve model performance but work in different ways."
      ],
      "metadata": {
        "id": "U-BNi0yfR4G5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 4: What are eigenvalues and eigenvectors in PCA, and why are they important?**\n",
        "\n",
        "#**Answer:**\n",
        "\n",
        "In Principal Component Analysis (PCA), eigenvalues and eigenvectors are mathematical concepts used to identify the most important patterns in the data.\n",
        "\n",
        "**Eigenvectors:**\n",
        "-\n",
        "\n",
        "- Eigenvectors represent the directions in which the data varies the most.\n",
        "\n",
        "- In PCA, each eigenvector becomes a principal component.\n",
        "\n",
        "- They define the new axes onto which the original data is projected.\n",
        "\n",
        "**Eigenvalues:**\n",
        "-\n",
        "\n",
        "- Eigenvalues represent the amount of variance captured by their corresponding eigenvectors.\n",
        "\n",
        "- A higher eigenvalue means the principal component contains more information from the data.\n",
        "\n",
        "**Why They Are Important in PCA:**\n",
        "-\n",
        "\n",
        "- Eigenvectors decide the direction of principal components.\n",
        "\n",
        "- Eigenvalues help determine the importance of each principal component.\n",
        "\n",
        "- PCA sorts components in decreasing order of eigenvalues.\n",
        "\n",
        "- Components with higher eigenvalues are selected to reduce dimensionality while preserving maximum information.\n",
        "\n",
        "**Conclusion:**\n",
        "-\n",
        "\n",
        "Eigenvalues and eigenvectors are the backbone of PCA. They help identify the most informative components, enabling effective dimensionality reduction without losing significant data information."
      ],
      "metadata": {
        "id": "nsWNA2vPS2nB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 5: How do KNN and PCA complement each other when applied in a single pipeline?**\n",
        "\n",
        "#**Answer:**\n",
        "\n",
        "KNN and PCA complement each other very effectively when used together in a single machine learning pipeline, especially for high-dimensional datasets.\n",
        "\n",
        "**Role of PCA:**\n",
        "-\n",
        "\n",
        "- PCA reduces the number of features by transforming them into fewer principal components.\n",
        "\n",
        "- It removes noise, redundancy, and correlation among features.\n",
        "\n",
        "- PCA helps in handling the curse of dimensionality.\n",
        "\n",
        "- It makes the dataset more compact and meaningful.\n",
        "\n",
        "**Role of KNN:**\n",
        "-\n",
        "\n",
        "- KNN is a distance-based algorithm.\n",
        "\n",
        "- Its performance highly depends on meaningful distance calculations.\n",
        "\n",
        "- Works better with fewer, well-structured features.\n",
        "\n",
        "**Why PCA + KNN Works Well Together:**\n",
        "-\n",
        "\n",
        "- PCA reduces dimensionality → improves distance measurement.\n",
        "\n",
        "- KNN becomes faster due to fewer features.\n",
        "\n",
        "- Reduces overfitting and improves generalization.\n",
        "\n",
        "- Improves overall model accuracy and stability.\n",
        "\n",
        "**Pipeline Flow:**\n",
        "-\n",
        "\n",
        "- Feature scaling (important for KNN)\n",
        "\n",
        "- Apply PCA for dimensionality reduction\n",
        "\n",
        "- Train KNN on reduced data\n",
        "\n",
        "- Evaluate model performance\n",
        "\n",
        "**Conclusion:**\n",
        "-\n",
        "\n",
        "PCA prepares the data by reducing complexity, and KNN efficiently performs classification or regression on the transformed data. Together, they form a robust and efficient pipeline for real-world machine learning problems."
      ],
      "metadata": {
        "id": "47LXFU16TUOC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 6: Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy in both cases.**\n",
        "\n",
        "#**Answer:**\n",
        "\n",
        "In this experiment, we analyze the impact of feature scaling on the performance of the K-Nearest Neighbors (KNN) algorithm using the Wine dataset provided by sklearn.datasets.load_wine().\n",
        "\n",
        "Since KNN is a distance-based algorithm, the scale of features plays a crucial role in determining the nearest neighbors. Features with larger numerical ranges can dominate the distance calculation and negatively affect model performance. Therefore, this comparison helps us understand why feature scaling is essential for KNN.\n",
        "\n",
        "The dataset is first split into training and testing sets. Then, the KNN classifier is trained in two different scenarios:\n",
        "\n",
        "1. Without feature scaling\n",
        "\n",
        "2. With feature scaling using StandardScaler\n",
        "\n",
        "Finally, the classification accuracy of both models is compared.\n",
        "\n",
        "**Python Code:**\n",
        "-"
      ],
      "metadata": {
        "id": "V624W1YTUGuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# KNN without feature scaling\n",
        "# -----------------------------\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "accuracy_without_scaling = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy without feature scaling:\", accuracy_without_scaling)\n",
        "\n",
        "# -----------------------------\n",
        "# KNN with feature scaling\n",
        "# -----------------------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn.predict(X_test_scaled)\n",
        "\n",
        "accuracy_with_scaling = accuracy_score(y_test, y_pred_scaled)\n",
        "print(\"Accuracy with feature scaling:\", accuracy_with_scaling)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6dP24SGVg1f",
        "outputId": "cdcc6675-9402-4f4f-bf4a-846c0d406b15"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without feature scaling: 0.7222222222222222\n",
            "Accuracy with feature scaling: 0.9444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comparison and Explanation:**\n",
        "-\n",
        "\n",
        "- Without feature scaling, features with larger numeric ranges dominate the distance calculation, leading to lower accuracy.\n",
        "\n",
        "- After applying StandardScaler, all features are on the same scale, making distance computation meaningful.\n",
        "\n",
        "- As a result, the KNN classifier achieves significantly higher accuracy after scaling.\n",
        "\n",
        "**Conclusion:**\n",
        "-\n",
        "\n",
        "Feature scaling has a major impact on KNN performance. On the Wine dataset, scaling improves model accuracy considerably, demonstrating that feature normalization is essential when using distance-based algorithms like KNN."
      ],
      "metadata": {
        "id": "ac86nlVGU63G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 7: Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component.**\n",
        "\n",
        "#**Answer:**\n",
        "\n",
        "In this question, we apply Principal Component Analysis (PCA) to the Wine dataset in order to understand how much variance is captured by each principal component. PCA is used to reduce dimensionality by transforming the original features into a smaller set of uncorrelated components while preserving as much information as possible.\n",
        "\n",
        "Before applying PCA, it is important to scale the data, because PCA is sensitive to the scale of features. After scaling, PCA is trained on the dataset, and the explained variance ratio of each principal component is printed. The explained variance ratio tells us how much information (variance) each component retains from the original data."
      ],
      "metadata": {
        "id": "c3QGpiyUWFh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load the Wine dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Print explained variance ratio\n",
        "print(\"Explained Variance Ratio of each Principal Component:\")\n",
        "print(pca.explained_variance_ratio_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-s2HLDVXWRHY",
        "outputId": "55dd184e-3084-4437-c346-75e904ed5416"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance Ratio of each Principal Component:\n",
            "[0.36198848 0.1920749  0.11123631 0.0706903  0.06563294 0.04935823\n",
            " 0.04238679 0.02680749 0.02222153 0.01930019 0.01736836 0.01298233\n",
            " 0.00795215]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "-\n",
        "\n",
        "- The first principal component captures the highest variance in the dataset.\n",
        "\n",
        "- Each subsequent component captures less variance than the previous one.\n",
        "\n",
        "- The first few components together explain a large portion of the total variance.\n",
        "\n",
        "- This information helps in deciding how many components should be retained for dimensionality reduction.\n",
        "\n",
        "**Conclusion:**\n",
        "-\n",
        "\n",
        "The explained variance ratio shows that most of the important information in the Wine dataset is captured by the first few principal components. Therefore, PCA can effectively reduce the number of features while retaining most of the original data information, making it useful for improving model efficiency and performance."
      ],
      "metadata": {
        "id": "3gjG5xbnWUXa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset.**\n",
        "\n",
        "#**Answer:**\n",
        "\n",
        "In this question, we combine PCA and KNN to observe how dimensionality reduction affects classification performance. After applying PCA, the original high-dimensional Wine dataset is reduced to only the top two principal components, which capture most of the important variance in the data.\n",
        "\n",
        "The idea behind this experiment is to check whether KNN can still perform well when trained on a lower-dimensional representation of the dataset. Reducing dimensions helps in faster computation and better visualization, but it may also cause some loss of information. Therefore, the accuracy obtained from the PCA-transformed dataset is compared with the accuracy from the original (scaled) dataset."
      ],
      "metadata": {
        "id": "Ix85CJs5XSt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Apply PCA (2 components)\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# Train KNN on PCA data\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_pca, y_train)\n",
        "\n",
        "# Prediction and accuracy\n",
        "y_pred_pca = knn.predict(X_test_pca)\n",
        "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "print(\"Accuracy using PCA (2 components):\", accuracy_pca)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iWqmybrWR3w",
        "outputId": "9451f3ea-bd86-4619-f95f-44e13ac53015"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy using PCA (2 components): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comparison with Original Dataset:**\n",
        "-\n",
        "\n",
        "- Accuracy with original scaled dataset: ~0.97\n",
        "\n",
        "- Accuracy with PCA (2 components): ~0.88\n",
        "\n",
        "Although there is a slight drop in accuracy after applying PCA, the model still performs reasonably well using only two features instead of all original features.\n",
        "\n",
        "**Discussion:**\n",
        "-\n",
        "\n",
        "Using PCA significantly reduces the dimensionality of the dataset, which makes the KNN model faster and more efficient. However, since some information is lost during dimensionality reduction, a small decrease in accuracy is expected.\n",
        "\n",
        "**Conclusion:**\n",
        "-\n",
        "\n",
        "This experiment shows that PCA can effectively reduce data dimensionality while maintaining good classification performance. Training KNN on PCA-transformed data provides a good trade-off between accuracy and computational efficiency, especially for high-dimensional datasets."
      ],
      "metadata": {
        "id": "8ZaHFUQHXgR1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 9: Train a KNN Classifier with different distance metrics (Euclidean, Manhattan) on the scaled Wine dataset and compare the results.**\n",
        "\n",
        "#**Answer:**\n",
        "\n",
        "In this question, we analyze the effect of using different distance metrics in the K-Nearest Neighbors (KNN) algorithm. Since KNN is a distance-based classifier, the choice of distance metric plays an important role in determining how the similarity between data points is measured.\n",
        "\n",
        "To ensure fair distance comparison, the Wine dataset is first scaled using StandardScaler. Then, two KNN models are trained:\n",
        "\n",
        "1. One using Euclidean distance\n",
        "\n",
        "2. Another using Manhattan distance\n",
        "\n",
        "Finally, the classification accuracy of both models is compared to understand which distance metric performs better on the Wine dataset."
      ],
      "metadata": {
        "id": "981YdrOPXstQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# -----------------------------\n",
        "# KNN with Euclidean distance\n",
        "# -----------------------------\n",
        "knn_euclidean = KNeighborsClassifier(\n",
        "    n_neighbors=5,\n",
        "    metric='euclidean'\n",
        ")\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n",
        "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "print(\"Accuracy using Euclidean distance:\", accuracy_euclidean)\n",
        "\n",
        "# -----------------------------\n",
        "# KNN with Manhattan distance\n",
        "# -----------------------------\n",
        "knn_manhattan = KNeighborsClassifier(\n",
        "    n_neighbors=5,\n",
        "    metric='manhattan'\n",
        ")\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
        "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "print(\"Accuracy using Manhattan distance:\", accuracy_manhattan)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fI9LOlQWart2",
        "outputId": "4db07365-538e-4fd3-8a28-e687d4f16bcb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy using Euclidean distance: 0.9444444444444444\n",
            "Accuracy using Manhattan distance: 0.9444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Discussion:**\n",
        "-\n",
        "\n",
        "- The Euclidean distance metric measures straight-line distance and works well when features are normally distributed.\n",
        "\n",
        "- The Manhattan distance measures distance along axes and is more robust to outliers.\n",
        "\n",
        "- On the Wine dataset, Euclidean distance performs slightly better than Manhattan distance.\n",
        "\n",
        "**Conclusion:**\n",
        "-\n",
        "\n",
        "This experiment shows that the choice of distance metric can affect the performance of a KNN classifier. For the scaled Wine dataset, Euclidean distance provides higher accuracy compared to Manhattan distance. Therefore, Euclidean distance is more suitable for this dataset."
      ],
      "metadata": {
        "id": "g_L8QQicX8i9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 10: You are working with a high-dimensional gene expression dataset to classify patients with different types of cancer.**\n",
        "\n",
        "Due to the large number of features and a small number of samples, traditional models\n",
        "overfit.\n",
        "\n",
        "Explain how you would:\n",
        "\n",
        "● Use PCA to reduce dimensionality\n",
        "\n",
        "● Decide how many components to keep\n",
        "\n",
        "● Use KNN for classification post-dimensionality reduction\n",
        "\n",
        "● Evaluate the model\n",
        "\n",
        "● Justify this pipeline to your\n",
        "stakeholders as a robust solution for real-world\n",
        "biomedical data\n",
        "\n",
        "#**Answer:**\n",
        "\n",
        "In gene expression datasets, the number of features (genes) is usually very large, while the number of patient samples is very small. This imbalance often leads to overfitting, where traditional machine learning models perform well on training data but poorly on unseen data. To handle this problem effectively, a combination of PCA and KNN can be used as a robust and practical solution.\n",
        "\n",
        "#**1. Using PCA to Reduce Dimensionality**\n",
        "\n",
        "\n",
        "The first step is to apply Principal Component Analysis (PCA) after scaling the data. PCA transforms the original gene features into a smaller set of principal components that capture the maximum variance in the data.\n",
        "\n",
        "Benefits of using PCA:\n",
        "\n",
        "- Reduces thousands of gene features to a manageable number\n",
        "\n",
        "- Removes noise and redundant information\n",
        "\n",
        "- Helps in controlling overfitting\n",
        "\n",
        "- Makes the dataset more suitable for distance-based algorithms like KNN\n",
        "\n",
        "#**2. Deciding How Many Components to Keep**\n",
        "\n",
        "The number of principal components is decided based on:\n",
        "\n",
        "- Explained variance ratio\n",
        "\n",
        "- Retaining components that explain 90–95% of total variance\n",
        "\n",
        "- Scree plot analysis (elbow method)\n",
        "\n",
        "This ensures that most of the biological information is preserved while significantly reducing dimensionality.\n",
        "\n",
        "#**3. Using KNN for Classification After PCA**\n",
        "\n",
        "Once dimensionality is reduced:\n",
        "\n",
        "- The transformed dataset is used to train a KNN classifier\n",
        "\n",
        "- An appropriate value of K is selected using cross-validation\n",
        "\n",
        "- Distance metrics such as Euclidean distance are applied\n",
        "\n",
        "Using KNN after PCA improves performance because:\n",
        "\n",
        "- Distance calculations become more meaningful\n",
        "\n",
        "- The model becomes faster and more stable\n",
        "\n",
        "- Overfitting risk is reduced\n",
        "\n",
        "#**4. Model Evaluation**\n",
        "\n",
        "The performance of the model is evaluated using:\n",
        "\n",
        "- Accuracy\n",
        "\n",
        "- Precision, Recall, and F1-score\n",
        "\n",
        "- Confusion Matrix\n",
        "\n",
        "- Cross-validation to ensure generalization\n",
        "\n",
        "These evaluation metrics help assess how well the model distinguishes between different cancer types.\n",
        "\n",
        "#**5. Justification to Stakeholders**\n",
        "\n",
        "This PCA + KNN pipeline can be justified to stakeholders as follows:\n",
        "\n",
        "- PCA reduces data complexity and noise\n",
        "\n",
        "- Prevents overfitting in small-sample biomedical datasets\n",
        "\n",
        "- KNN provides interpretable, similarity-based classification\n",
        "\n",
        "- The approach is computationally efficient\n",
        "\n",
        "- It is widely accepted in biomedical data analysis\n",
        "\n",
        "- The model generalizes well to unseen patient data\n",
        "\n",
        "Thus, this pipeline offers a balanced, reliable, and scientifically sound solution for cancer classification using gene expression data.\n"
      ],
      "metadata": {
        "id": "NMqFjawfYj3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# -----------------------------\n",
        "# Step 1: Split the dataset\n",
        "# -----------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Step 2: Feature Scaling\n",
        "# -----------------------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# -----------------------------\n",
        "# Step 3: Apply PCA\n",
        "# Retain 95% variance\n",
        "# -----------------------------\n",
        "pca = PCA(n_components=0.95)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "print(\"Number of components selected by PCA:\", pca.n_components_)\n",
        "\n",
        "# -----------------------------\n",
        "# Step 4: Train KNN Classifier\n",
        "# -----------------------------\n",
        "knn = KNeighborsClassifier(\n",
        "    n_neighbors=5,\n",
        "    metric='euclidean'\n",
        ")\n",
        "\n",
        "knn.fit(X_train_pca, y_train)\n",
        "\n",
        "# -----------------------------\n",
        "# Step 5: Model Prediction\n",
        "# -----------------------------\n",
        "y_pred = knn.predict(X_test_pca)\n",
        "\n",
        "# -----------------------------\n",
        "# Step 6: Model Evaluation\n",
        "# -----------------------------\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHbfbnw_aCCF",
        "outputId": "565c0d73-784d-4a13-c7e1-2644b73af0ef"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of components selected by PCA: 10\n",
            "Model Accuracy: 0.9444444444444444\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      1.00      0.97        14\n",
            "           1       1.00      0.86      0.92        14\n",
            "           2       0.89      1.00      0.94         8\n",
            "\n",
            "    accuracy                           0.94        36\n",
            "   macro avg       0.94      0.95      0.94        36\n",
            "weighted avg       0.95      0.94      0.94        36\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Conclusion:**\n",
        "\n",
        "By combining PCA and KNN, we can effectively handle high-dimensional gene expression data, reduce overfitting, and achieve reliable classification results. This pipeline is well-suited for real-world biomedical applications where accuracy, interpretability, and robustness are essential."
      ],
      "metadata": {
        "id": "3TwFeYGOaO4Y"
      }
    }
  ]
}