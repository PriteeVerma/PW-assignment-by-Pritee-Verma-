{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Supervised Classification: Decision Trees, SVM, and Naive Bayes| Assignment**"
      ],
      "metadata": {
        "id": "JSNK41QOm-MX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 1: What is Information Gain, and how is it used in Decision Trees?**\n",
        "\n",
        "#**Answer:**\n",
        "\n",
        "Information Gain is a metric used in Decision Trees to measure how well a feature separates the data into different classes. It is based on the concept of Entropy, which measures the level of impurity or uncertainty in a dataset.\n",
        "\n",
        "Information Gain calculates the reduction in entropy after splitting the dataset on a particular feature. The feature that provides the highest Information Gain is selected as the splitting node in the Decision Tree.\n",
        "\n",
        "In simple words, Information Gain helps the Decision Tree decide which feature is the best to split the data so that the classes become as pure as possible.\n",
        "\n",
        "#**Question 2: What is the difference between Gini Impurity and Entropy?**\n",
        "\n",
        "#**Answer:**\n",
        "\n",
        "Gini Impurity and Entropy are both measures used to evaluate the impurity of a node in Decision Trees.\n",
        "\n",
        "Gini Impurity measures the probability that a randomly chosen data point would be incorrectly classified. It is computationally faster and commonly used in CART (Classification and Regression Trees).\n",
        "\n",
        "Entropy measures the level of disorder or randomness in the data. It comes from information theory and is used in ID3 and C4.5 algorithms.\n",
        "\n",
        "While both aim to create pure nodes, Gini Impurity is simpler and faster, whereas Entropy is more mathematically informative.\n",
        "\n",
        "#**Question 3: What is Pre-Pruning in Decision Trees?**\n",
        "\n",
        "#**Answer:**\n",
        "\n",
        "Pre-Pruning is a technique used to control the growth of a Decision Tree during the training process in order to prevent overfitting. Instead of allowing the tree to grow fully, pre-pruning applies certain stopping conditions early.\n",
        "\n",
        "Common pre-pruning methods include limiting the maximum depth of the tree, specifying the minimum number of samples required to split a node, or setting a minimum threshold for impurity reduction.\n",
        "\n",
        "By restricting tree growth, pre-pruning reduces model complexity, improves generalization on unseen data, and helps maintain better prediction performance.\n",
        "\n",
        "#**Question 4: Write a Python program to train a Decision Tree Classifier using Gini Impurity and print feature importances.**"
      ],
      "metadata": {
        "id": "fZcOWEb8niYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train Decision Tree with Gini Impurity\n",
        "model = DecisionTreeClassifier(criterion='gini')\n",
        "model.fit(X, y)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\", model.feature_importances_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8mDh9D2oK5T",
        "outputId": "f9ffbf94-8300-46c7-c0df-471981938e14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances: [0.01333333 0.01333333 0.05072262 0.92261071]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This program trains a Decision Tree using the Gini criterion and displays the importance of each feature in making decisions."
      ],
      "metadata": {
        "id": "bGePtex8o-7y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 5: What is a Support Vector Machine (SVM)?**\n",
        "\n",
        "#**Answer:**\n",
        "\n",
        "Support Vector Machine (SVM) is a supervised learning algorithm mainly used for classification tasks. It works by finding an optimal decision boundary, known as a hyperplane, that best separates data points of different classes.\n",
        "\n",
        "SVM aims to maximize the margin, which is the distance between the hyperplane and the closest data points from each class, called support vectors. By maximizing this margin, SVM achieves better generalization and robustness, especially in high-dimensional datasets.\n",
        "\n",
        "#**Question 6: What is the Kernel Trick in SVM?**\n",
        "\n",
        "#**Answer:**\n",
        "\n",
        "The Kernel Trick is a technique used in SVM to classify non-linearly separable data. Instead of separating data directly in the original feature space, the kernel trick maps the data into a higher-dimensional space where linear separation becomes possible.\n",
        "\n",
        "Popular kernels include Linear, Polynomial, and Radial Basis Function (RBF). The key advantage of the kernel trick is that it performs this transformation without explicitly computing the higher-dimensional features, making SVM computationally efficient and powerful.\n",
        "\n",
        "#**Question 7: Write a Python program to train Linear and RBF SVM classifiers on the Wine dataset and compare accuracies.**\n",
        "\n",
        "#**Answer:**"
      ],
      "metadata": {
        "id": "VQJ-DEU-pA0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Linear SVM\n",
        "linear_svm = SVC(kernel='linear')\n",
        "linear_svm.fit(X_train, y_train)\n",
        "linear_pred = linear_svm.predict(X_test)\n",
        "\n",
        "# RBF SVM\n",
        "rbf_svm = SVC(kernel='rbf')\n",
        "rbf_svm.fit(X_train, y_train)\n",
        "rbf_pred = rbf_svm.predict(X_test)\n",
        "\n",
        "# Compare accuracies\n",
        "print(\"Linear SVM Accuracy:\", accuracy_score(y_test, linear_pred))\n",
        "print(\"RBF SVM Accuracy:\", accuracy_score(y_test, rbf_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3pCRnIfphjB",
        "outputId": "ace943d8-372a-4f06-f174-2004b72ccf0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear SVM Accuracy: 0.9722222222222222\n",
            "RBF SVM Accuracy: 0.6388888888888888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "This code compares the performance of Linear and RBF kernels on the same dataset."
      ],
      "metadata": {
        "id": "J15Kb8aIplNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?**\n",
        "\n",
        "#**Answer:**\n",
        "Naïve Bayes is a probabilistic classification algorithm based on Bayes’ Theorem. It assumes that all features are independent of each other given the class label.\n",
        "\n",
        "This assumption is usually unrealistic in real-world data, which is why the algorithm is called “Naïve”. However, despite this limitation, Naïve Bayes is simple, fast, and highly effective in applications such as spam detection, sentiment analysis, and document classification.\n",
        "\n",
        "#**Question 9: Explain the differences between Gaussian, Multinomial, and Bernoulli Naïve Bayes.**\n",
        "\n",
        "#**Answer:**\n",
        "\n",
        "Gaussian Naïve Bayes is used when the input features are continuous and follow a normal distribution. It is commonly applied in medical and scientific datasets.\n",
        "\n",
        "Multinomial Naïve Bayes is suitable for discrete count data, such as word frequencies in text classification tasks.\n",
        "\n",
        "Bernoulli Naïve Bayes is designed for binary features, where features represent the presence or absence of a characteristic.\n",
        "\n",
        "Each variant is selected based on the nature of the input data.\n",
        "\n",
        "#**Question 10: Train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy.**\n",
        "\n",
        "#**Answer:**"
      ],
      "metadata": {
        "id": "9Y-lQOtJpmmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Train Gaussian Naïve Bayes\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "TLp9Y6krqC7q",
        "outputId": "e5b3d36b-3f8e-4563-8400-6edef5c8d395",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9473684210526315\n"
          ]
        }
      ]
    }
  ]
}